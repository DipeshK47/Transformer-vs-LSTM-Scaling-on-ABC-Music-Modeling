Scaling analysis summary
========================

Per model metrics (ordered by size):

model_size, params, val_loss_after_1_epoch, epoch_time_min, epoch_time_sec, peak_gpu_mem_gb
tiny, 1010000, 1.2026, 0.82, 49.199999999999996, 1.17
small, 4890000, 1.0821, 2.26, 135.6, 3.66
medium, 19570000, 0.9337, 3.52, 211.2, 5.68
large, 49600000, 0.8564, 5.2, 312.0, 8.31
xl, 100280000, 0.7965, 7.46, 447.6, 11.66

Power law fit: L = a * N^(-alpha) + c
a ≈ 4.302806
alpha ≈ 0.091091
c ≈ 0.000000

Implications:
The exponent alpha controls how fast validation loss decreases as model size increases. For example, doubling the number of parameters changes the loss by roughly a factor of 2^(-alpha). A smaller alpha means diminishing returns from increasing model size, while a larger alpha would indicate stronger gains from scaling the model.
